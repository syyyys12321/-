# 梯度下降

## 第度下降

对于 $J(\omega,b)$ ,需要设计一个算法，来帮我们系统的找到它的最小值。现在，我用 $\omega$ 的更新为例说明这件事情。首先，需要取 $J(\omega,b)$ 的偏导数

$$
  J_{\omega} = \frac{\partial J(\omega,b)}{\partial \omega}
$$

这个偏导数可以帮助我们判断，需要增加 $\omega$ 还是减少 $\omega$ 。如果 $J_{\omega} > 0$ ,那么说明增加 $\omega$ 会增加 $J(\omega,b)$ ，如果 $J_{\omega} > 0$ ，需要减小 $\omega$  ,反过来也成立。于是有

$$
  \omega_{new} = \omega_{old} - \alpha J_{\omega} 
$$

其中， $\alpha$ 为学习率，可以控制 $\omega$ 变化的速度，合适的学习率设置对于机器学习至关重要。可以采取同样的方法更新所有的参数。梯度下降可以拓展至高纬空间。

## 特征缩放

如果拟合函数并不是简单的线性函数，而是如下形式：

$$
  y = \omega_1 x_1 + \omega_2 x_2 + b
$$

那么我们即有两个需要调节的参数，这两个参数很有可能差别巨大。例如，当 $x_1 \in (0,1)$ ， $x_2 \in (0,100)$ 时，因为拟合函数中前两项的贡献相当，则 $\omega_1 >> \omega_2$ 。那么在拟合这两个参数的时候， $\omega_2$ 会产生振荡，难以收敛。将 $x_1$ 与 $x_2$ 进行正则化可以解决这个问题。譬如：

$$
x_1^{\prime} = x_1 
$$

$$
x_2^{\prime} = x_2/100
$$


