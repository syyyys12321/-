# 5-1

```python
import torch
from torch import nn
import d2l
```

## 如果将MySequential中存储块的方式更改为Python列表，会出现什么样的问题？


```python
x=torch.ones(1,5)
# 更改前
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员
            # 变量_modules中。_module的类型是OrderedDict
            self._modules[str(idx)] = module
    def forward(self, X):
        # OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            X = block(X)
        return X
def init_constant(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight,1)
        nn.init.zeros_(m.bias)
a=MySequential(nn.Linear(5, 5), nn.ReLU(), nn.Linear(5, 5))
a.apply(init_constant)
print(a(x))
print(a.state_dict())
```

    tensor([[25., 25., 25., 25., 25.]], grad_fn=<AddmmBackward0>)
    OrderedDict([('0.weight', tensor([[1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 1.]])), ('0.bias', tensor([0., 0., 0., 0., 0.])), ('2.weight', tensor([[1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 1.]])), ('2.bias', tensor([0., 0., 0., 0., 0.]))])



```python
# 更改后
class MySequential2(nn.Module):
    def __init__(self, *args):
        super().__init__()
        self.list=[]
        for idx, module in enumerate(args):
            self.list.append(module)
    def forward(self, X):
        for block in self.list:
            X = block(X)
        return X
        
def init_constant(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight,1)
        nn.init.zeros_(m.bias)
a2=MySequential2(nn.Linear(5, 5), nn.ReLU(), nn.Linear(5, 5))
# 无法使用自带的apply初始化参数了
a2.apply(init_constant)
print(a2(x))
# 需要这样初始化，或者重写apply
for i in a2.list:
    init_constant(i)
print(a2(x))
# state_dict() 失效了
print(a2.state_dict())
```

    tensor([[ 0.4897, -0.3354, -0.1935,  0.4090, -0.2873]],
           grad_fn=<AddmmBackward0>)
    tensor([[25., 25., 25., 25., 25.]], grad_fn=<AddmmBackward0>)
    OrderedDict()


## 实现一个块，它以两个块为参数，例如net1和net2，并返回前向传播中两个网络的串联输出。这也被称为平行块。


```python
class ParallelSequential(nn.Module):
    def __init__(self,*args):
        super().__init__()
        for idx,module in enumerate(args):
            self._modules[str(idx)]=module
    def forward(self, X):
        return torch.cat((self._modules['0'](X),self._modules['1'](X)),dim=0)
net1=MySequential(nn.Linear(5,5), nn.ReLU(), nn.Linear(5,5))
net2=MySequential(nn.Linear(5,5), nn.ReLU(), nn.Linear(5,5))
a3=ParallelSequential(net1,net2)
a3.apply(init_constant)
a3(x)
```




    tensor([[25., 25., 25., 25., 25.],
            [25., 25., 25., 25., 25.]], grad_fn=<CatBackward0>)



## 假设我们想要连接同一网络的多个实例。实现一个函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。


```python
# 不知道这个问题是什么意思
```



# 5-4


```python
import torch as t
from torch import nn
```

## 设计一个接受输入并计算张量降维的层，它返回 $y_k =\sum_{i,j} w_{ijk}x_ix_j $。


```python
x=t.rand(1,50)
class MyNet(nn.Module):
    def __init__(self,i,k):
        super().__init__()
        self.w=t.rand(k,i,i)
    def forward(self,X):
        return t.matmul(t.matmul(X,self.w),X.T).resize(1,len(self.w))
net1=MyNet(50,3)
net1(x)
```




    tensor([[358.1713, 363.2960, 365.2417]])



## 设计一个返回输入数据的傅立叶系数前半部分的层。


```python
y=t.rand(5,10)
class FftLayer(nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self,X):
        return t.fft.fft(X)[:,:int(X.shape[1]/2)]
net2=FftLayer()
net2(y)
```




    tensor([[ 6.3819+0.0000j,  0.1688-0.6493j,  1.3128+0.6818j,  0.2078-0.1599j,
             -0.3696-0.4462j],
            [ 4.9797+0.0000j,  1.1539-0.7108j, -0.6577+0.0074j,  0.5322-0.8764j,
              0.9387-1.0444j],
            [ 6.0185+0.0000j,  0.1712-0.3071j,  0.2936+0.2553j, -0.1519-0.2267j,
             -0.8908-0.1045j],
            [ 3.6282+0.0000j, -0.9893+0.5866j,  0.9151-0.7108j, -0.6939+0.2605j,
              0.3561-1.2078j],
            [ 5.1771+0.0000j, -0.2901-1.0388j,  0.4601-0.6504j,  0.0841-0.9869j,
              0.7041+1.5799j]])

# 5-5

```python
import torch as t
from torch import nn
```

试一下利用torch存储数据


```python
net = nn.Sequential(nn.Linear(5,5),nn.ReLU())
def init(m):
    if type(m) == nn.Linear:
        nn.init.uniform_(m.weight,a=0,b=1)
net.apply(init)
t.save(net.state_dict(),"net_dict")
net2 = nn.Sequential(nn.Linear(5,5),nn.ReLU())
net2.load_state_dict(t.load("net_dict",weights_only=False))
x=t.tensor([[2,3,4,5,6]],dtype=t.float)
net2(x)==net(x)
```




    tensor([[True, True, True, True, True]])



## 5-5练习

### 即使不需要将经过训练的模型部署到不同的设备上，存储模型参数还有什么实际的好处？
和备份数据一样，防止因为数据丢失或者损坏带来的损失。还能起到‘git’的作用，存储数据，方便在模型训练后进行回朔、对比等。
### 假设我们只想复用网络的一部分，以将其合并到不同的网络架构中。比如想在一个新的网络中使用之前网络的前两层，该怎么做？


```python
net_long=nn.Sequential(nn.Linear(5,5),nn.ReLU(),nn.Linear(5,2))
# 直接保存整个网络更方便
t.save(net_long,"net_long")
net3 = nn.Sequential(nn.Linear(5,5),nn.ReLU(),nn.Linear(5,2))
net3[:2].load_state_dict(t.load("net_long",weights_only=False)[:2].state_dict())
print(net_long[0:2](x)==net3[0:2](x))
print(net_long(x)==net3(x))
```

    tensor([[True, True, True, True, True]])
    tensor([[False, False]])


## 如何同时保存网络架构和参数？需要对架构加上什么限制？
目前没想到有什么限制
